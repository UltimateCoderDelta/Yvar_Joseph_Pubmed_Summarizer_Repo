# -*- coding: utf-8 -*-
"""Text_Summarization_Final_updated.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IMSfvHBo-hmvkrrO3o0yLswgj8YhXapW

Developer name: Yvar Joseph

Project: Summarizer Model for Healthcare Documentation
"""

!pip install evaluate #To evaluate various pre-trained models on our dataset
!pip install rouge #To evaluate the rouge metrics for our summarization models
!pip install sacrebleu #To evaluate the sacrebleu metrics for our summarization models

from datasets import load_dataset
import numpy as np
import pandas as pd

"""Loading the dataset for evaluation"""

#Import
import datasets
from datasets import load_dataset

pubmed_ds = load_dataset("ronitHF/pubmed-10k-8.1.1")

pubmed_ds

#Performing data splits
pubmed_ds_train = pubmed_ds["train"]
pubmed_ds_valid = pubmed_ds["validation"]

#Evaluating the columns of our dataset and their respective lengths
print(f'Dataset features: {pubmed_ds_train.column_names}')
print(f'Length of our training set: {len(pubmed_ds_train)}')
print(f'Length of our validation set: {len(pubmed_ds_valid)}')

#Peeking into the first 500 characters of the training set's articles
sample_article_iter = pubmed_ds_train["article"][0][:500]
sample_asbtract_iter = pubmed_ds_train["abstract"][0]
sample_article, sample_abstract = sample_article_iter, sample_asbtract_iter
print(f"""
SAMPLE ARTICLE AND ABSTRACTION FOR THE PUBMED DATASET

First 500 characters of sample article | Total Size: {len(pubmed_ds_train["article"][0])}
===============================================================================
{sample_article}
===============================================================================
Sample Summary

{sample_abstract}
""")

"""Phase 2 - Preprocessing the dataset prior to tokenization

"""

from datasets import Dataset
#For a clean preprocessing of the dataset, the Dataset object will be converted to a pandas Dataframe
print(f'Dataset object type', type(pubmed_ds_train))
#Converting the Dataset object to a pandas dataframe
pubmed_df = pubmed_ds_train.to_pandas()
#Evaluate the first five rows of data
pubmed_df.head()

pubmed_df_valid = pubmed_ds_valid.to_pandas()

pubmed_df.columns

#Dropping undeed columns
pubmed_df.drop(columns=["section_names"], inplace=True)

pubmed_df_valid.drop(columns=["section_names"], inplace=True)

pubmed_df.head()

#Checking the dataframe for null values
check_nulls = lambda x: x.isnull().sum() > 0
pubmed_df.apply(check_nulls)

check_nulls_valid = lambda x: x.isnull().sum() > 0
pubmed_df_valid.apply(check_nulls_valid)

#Since the validation dataset has no null values, we can return it to its dataset format
from datasets import Dataset
pubmed_ds_valid = Dataset.from_pandas(pubmed_df_valid)

#Removing nan rows
pubmed_df.dropna(inplace=True)

len(pubmed_df)

#Analyzing the first 3000 words of a sample article
sample_article = pubmed_df["article"][1][:3000]
sample_article

import nltk
import re

def preprocessor(dataset):
    dataset = dataset.lower()
    dataset = re.sub(r'(background\s+:)|(conclusion\s+:)|(\n)', '', dataset)
    dataset = re.sub(r'\(\s*\w+(\s+\w+)*\s*\)|\(\w+(\s+.|\S)\d\)', '', dataset)
    dataset = re.sub(r'\([^)]*\)', '', dataset)
    dataset = re.sub(r'[\?\:\!\;\[\]\*\<\>]', '', dataset)
    dataset = re.sub(r'\.(\d+,\d+|\d+)', '.', dataset)
    dataset = dataset.replace('.', ' . ')
    dataset = dataset.replace('%', ' % ')
    found_pattern = re.findall(r'(\w+\s+/\s+\w+\s+)', dataset)[:]
    for pattern in found_pattern:
        if pattern in dataset:
           pattern_updated = pattern.replace('/', 'per')
           dataset = dataset.replace(pattern, pattern_updated)

    return dataset.strip()

preprocessor(sample_article) #Testing the dummy preprocessor function

#Developing a preprocessor function
import re
def preprocessor(dataset, feature="article"):
    dataset[feature] = dataset[feature].lower()
    dataset[feature] = re.sub(r'(background\s+:)|(conclusion\s+:)|(\n)', '', dataset[feature])
    dataset[feature] = re.sub(r'\(\s*\w+(\s+\w+)*\s*\)|\(\w+(\s+.|\S)\d\)', '', dataset[feature])
    dataset[feature] = re.sub(r'\([^)]*\)', '', dataset[feature])
    dataset[feature] = re.sub(r'[\?\:\!\;\[\]\*\<\>]', '', dataset[feature])
    dataset[feature] = re.sub(r'\.(\d+,\d+|\d+)', '.', dataset[feature])
    dataset[feature] = dataset[feature].replace('.', ' . ')
    dataset[feature] = dataset[feature].replace('%', ' % ')
    found_pattern = re.findall(r'(\w+\s+/\s+\w+\s+)', dataset[feature])[:]
    for pattern in found_pattern:
        if pattern in dataset[feature]:
           pattern_updated = pattern.replace('/', 'per')
           dataset[feature] = dataset[feature].replace(pattern, pattern_updated)
    return dataset

preprocessor(pubmed_ds["train"][1])["article"][:2000] #Testing the preprocessor on a sampl

# #Reducing the size of the dataset to 30% for training purposes
# import math
# from datasets import Dataset

# pubmed_data_train_mini = pubmed_ds[:math.floor(len(pubmed_ds["article"]) * 0.30)]
# print(len(pubmed_data_train_mini))
# #Convert this reduced dataframe to a dataset again
# pubmed_ds = Dataset.from_pandas(pubmed_data_train_mini)
# pubmed_ds

#Convert dataframe back to dataset for preprocessing implementation
from datasets import Dataset

updated_ds = Dataset.from_pandas(pubmed_df)

updated_ds["article"][1][:2000]

#Applying the preprocessor to the overall dataset
updated_ds = updated_ds.map(lambda x: preprocessor(x, feature="article"))
updated_ds = updated_ds.map(lambda x: preprocessor(x, feature="abstract"))

updated_ds["abstract"][1][:900] #testing the preprocessed data

pubmed_ds_valid = pubmed_ds_valid.map(lambda x: preprocessor(x, feature="article"))
pubmed_ds_valid = pubmed_ds_valid.map(lambda x: preprocessor(x, feature="abstract"))

print(pubmed_ds_valid["article"][1][:500])
print("\n\nUpdated abstract\n")
print(pubmed_ds_valid["abstract"][1])

"""Phase 3 - Now that the dataset has been preprocessed as best as possible, the next phase will focus on building a baseline
for the various pre-trained models we will be using for accuracy testing. For sample testing, we will make use of two models:

1) Google's Pegasus

2) Facebook's T5 transformer model
"""

sample_text = updated_ds["article"][1][:2000]
#Adding the various model summaries in a dictionary
summaries = {}

#Using the NLP convetion: seperating the summaries by a newline using NLTK
import nltk
from nltk.tokenize import sent_tokenize

nltk.download("punkt_tab")

#Testing the string
test_string = "The U.S. are a country. The U.N. is an organization"
sent_tokenize(test_string)

"""For the medical dataset analysis, we will be using a baseline for summarization: using the first four sentences of the document"""

def four_sentence_summary(document):
  return "\n".join(sent_tokenize(document)[:3])
summaries["baseline"] = four_sentence_summary(sample_text) #testing the baseline with the test (sample) string

summaries

"""PEGASUS MODEL TESTING"""

from transformers import pipeline

pegasus_checkpoint = "google/pegasus-xsum"
pipe = pipeline("summarization", model=pegasus_checkpoint)
pipe_out = pipe(sample_text)
summaries["pegasus"] = pipe_out[0]["summary_text"].replace("  .<n>", ".\n")

summaries["pegasus"]

"""BART MODEL TESTING"""

from transformers import pipeline

pipe = pipeline("summarization", model="facebook/bart-large-cnn")
pipe_out = pipe(sample_text)
summaries["bart"] = "\n".join(sent_tokenize(pipe_out[0]["summary_text"]))

summaries["bart"]

"""T5 MODEL TESTING"""

# pipe = pipeline("summarization", model="t5-large")
# pipe_out = pipe(sample_text)
# summaries["t5"] = "\n".join(sent_tokenize(pipe_out[0]["summary_text"]))

# summaries["t5"]

"""Phase 4 - Now that we have attained each of the respective summaries of our models, we can now compare their respective results

From the results above, it can be seen that BART seems to perform the best with respect to producing the most accurate summaries.
"""

print("GROUND TRUTH")
print(updated_ds["abstract"][1])
print("")

for model_name in summaries:
  print(model_name.upper())
  print(summaries[model_name])
  print("")

"""Model Evaluation Phase"""

#Measuring the quality of the generated text with BLEU
import evaluate

bleu_metric = evaluate.load("sacrebleu")

import pandas as pd
import numpy as np
#Performing a dummy prediction
bleu_metric.add(
    prediction="this is a dummy prediction",
    reference=["this dummy prediction"])
results = bleu_metric.compute(smooth_method="floor",
                              smooth_value=0)

results["precisions"] = [np.round(p, 2) for p in results["precisions"]]
pd.DataFrame.from_dict(results, orient="index", columns=["Value"])

#Dummy prediction 2
bleu_metric.add(
    predictions="the cat is on living room mat",
    references=["the cat is living room mat"])
results = bleu_metric.compute(smooth_method="floor", smooth_value=0)
results["precisions"] = [np.round(p, 2) for p in results["precisions"]]
pd.DataFrame.from_dict(results, orient="index", columns=["Value"])

"""Conclusion: We can see from the dummy predicition above, that
that our bleu score performs accurately on the 1-gram, less so
on the 2-gram, and is inneffective on 3 and 4 grams. Which makes sense given that the prediction and reference sentences do not match completely. This would somewhat mean that our model would be more precise on 1-grams (individual words) as opposed to multi-grams.

Performing calculations with ROUGE
"""

!pip install rouge_score

rouge_metric = evaluate.load("rouge")
reference = updated_ds["abstract"][1]
records = []
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]

for model_name in summaries:
  rouge_metric.add(prediction=summaries[model_name], reference=reference)
  score = rouge_metric.compute()
  rouge_dict = dict((rn, score[rn]) for rn in rouge_names)
  records.append(rouge_dict)
pd.DataFrame.from_records(records, index=summaries.keys())

"""Evaluating the [Selected Model] on the PubMed Dataset"""

def evulauate_summaries_baseline(dataset, metric,
                                 column_text="article",
                                 column_summary="abstract"):
  summaries= [four_sentence_summary(text) for text in dataset[column_text]]
  print(summaries)
  metric.add_batch(predictions=summaries,
                   references=dataset[column_summary])
  score = metric.compute()
  return score

test_sampled = pubmed_ds_valid.shuffle(seed=42).select(range(500))
score = evulauate_summaries_baseline(test_sampled, rouge_metric)
rouge_dict = dict((rn, score[rn]) for rn in rouge_names)
pd.DataFrame.from_dict(rouge_dict, orient="index", columns=["baseline"]).T

from tqdm import tqdm
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

def chunks(list_elements, batch_size):
  for i in range(0, len(list_elements), batch_size):
    yield list_elements[i : i + batch_size]

def evaluate_summaries_bart(dataset,
                            metric,
                            model,
                            tokenizer,
                            batch_size=16,
                            device=device,
                            column_text="article",
                            column_summary="abstract"):
  article_batches = list(chunks(dataset[column_text], batch_size))
  target_batches = list(chunks(dataset[column_summary], batch_size))

  for article_batch, target_batch in tqdm (
      zip(article_batches, target_batches), total=len(article_batches)):
      inputs = tokenizer(article_batch, max_length=1020,
                         truncation=True,
                         padding="max_length",
                         return_tensors="pt") #generate the sequences of our article batches
      summaries = model.generate(input_ids=inputs["input_ids"].to(device),
                                 attention_mask=inputs["attention_mask"].to(device),
                                 length_penalty=0.8,
                                 num_beams=8,
                                 max_length=300) #max generation length
      decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,
                                            clean_up_tokenization_spaces=True)
                                            for s in summaries]


      #Performing the overall analysis with our metric (rouge_score)
      decoded_summaries = ["\n".join(sent_tokenize(d)) for d in decoded_summaries] # Corrected line
      metric.add_batch(predictions=decoded_summaries, references=target_batch)

  score = metric.compute()
  return score

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

model_checkpoint = "facebook/bart-large-cnn"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device) # Move model to CPU

test_sampled = pubmed_ds_valid.shuffle(seed=42).select(range(100))

test_sampled

score = evaluate_summaries_bart(test_sampled, rouge_metric,
                                model, tokenizer, batch_size=6)
rouge_dict = dict((rn, score[rn]) for rn in rouge_names)

pd.DataFrame(rouge_dict, index=["bart"])

# #Training our summarization model
# import matplotlib.pyplot as plt


# d_len = [len(tokenizer.encode(s)) for s in updated_ds["article"]]
# s_len = [len(tokenizer.encode(s)) for s in updated_ds["abstract"]]

# fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)
# axes[0].hist(d_len, bins=20, color="C0", edgecolor="C0")
# axes[0].set_title("Dialogue Token Length")
# axes[0].set_xlabel("Length")
# axes[0].set_ylabel("Count")
# axes[1].hist(s_len, bins=20, color="C0", edgecolor="C0")
# axes[1].set_title("Summary Token Length")
# axes[1].set_xlabel("Length")
# plt.tight_layout()
# plt.show()

#Initiating the model training
def convert_examples_to_sequences(batch):
  input_encodings = tokenizer(batch["article"],
                              max_length=1024,
                              truncation=True)

  target_encodings = tokenizer(text_target=batch["abstract"], #adds special tokens to the summarizers
                                 max_length=300,
                                 truncation=True)
  return {
      #collect the sequences
      "input_ids": input_encodings["input_ids"],
      "attention_mask": input_encodings["attention_mask"],
      "labels": target_encodings["input_ids"]
  }

dataset_pubmed = updated_ds.map(convert_examples_to_sequences,
                                  batched=True)
dataset_pubmed_eval = pubmed_ds_valid.map(convert_examples_to_sequences,
                                             batched=True)
columns = ["input_ids", "labels", "attention_mask"]
dataset_pubmed.set_format(type="torch", columns=columns)

from transformers import DataCollatorForSeq2Seq

seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer

training_args = Seq2SeqTrainingArguments(
    output_dir='bart_pubmed_dataset_mark2',
    num_train_epochs=1,
    warmup_steps=400,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    weight_decay=0.01,
    logging_steps=10,
    push_to_hub=True,
    hub_model_id="Pubmed_Summarizer_Model_Trained",
    eval_strategy='steps',
    eval_steps=400,
    save_steps=1e6,
    predict_with_generate=True,
    gradient_accumulation_steps=16,
    report_to="none"
)

from huggingface_hub import notebook_login

notebook_login()

trainer = Seq2SeqTrainer(model=model, args=training_args,
                  tokenizer=tokenizer,
                  data_collator=seq2seq_data_collator,
                  train_dataset=dataset_pubmed,
                  eval_dataset=dataset_pubmed_eval,
                  )

trainer.train()

pubmed_valid_mini = pubmed_ds_valid[ :round(len(pubmed_ds_valid) * 0.20)]

pubmed_ds_valid_mini = pubmed_ds_valid.to_pandas()[:round(len(pubmed_ds_valid) * 0.10)]
pubmed_ds_valid_mini = Dataset.from_pandas(pubmed_ds_valid_mini)
pubmed_ds_valid_mini[0]["abstract"]

score = evaluate_summaries_bart(pubmed_ds_valid_mini,
                                rouge_metric,
                                trainer.model,
                                tokenizer,
                                batch_size=2)

score

sample_text_pred = """
His whole planet was destroyed. He's the last of a holocaust. He grew up in the dirt. Finding out slowly how different he was. A stranger discovering every day how strange he was. He has the power to tear the world apart. And he could. With a pinkie. It's not his world. We're not his people. We should be ants to him. Imagine that. Always being on the outside. The pain that would come from always being on the outside. And yet, he took that pain and became the symbol of hope

"""

sequenced_text = tokenizer([pubmed_ds_valid[1]["article"]],
                            max_length=1024,
                            truncation=True,
                            return_tensors="pt")
summary = trainer.model.generate(sequenced_text["input_ids"].to(trainer.model.device),
                       attention_mask=sequenced_text["attention_mask"].to(trainer.model.device),
                       length_penalty=0.8,
                       num_beams=8,
                       max_length=128)
decoded_summary = tokenizer.decode(summary[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)
decoded_summary

#Push the model to the hub
trainer.push_to_hub("Pubmed_Summarizer_Model_Mark2")

# Load model directly
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = AutoTokenizer.from_pretrained("AIEngineerYvar/Pubmed_Summarizer_Model_Trained")
model = AutoModelForSeq2SeqLM.from_pretrained("AIEngineerYvar/Pubmed_Summarizer_Model_Trained").to(device)

model.device

sequenced_text = tokenizer([sample_text_superman],
                            max_length=300,
                            truncation=True,
                            return_tensors="pt")
summary = model.generate(sequenced_text["input_ids"].to(model.device),
                       attention_mask=sequenced_text["attention_mask"].to(model.device),
                       length_penalty=0.8,
                       num_beams=8,
                       max_length=300)
decoded_summary = tokenizer.decode(summary[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)
decoded_summary